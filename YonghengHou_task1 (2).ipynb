{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#student_name:YonghengHou\n",
    "#student_number:5556661\n",
    "#login:yh790\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://windows10.microdone.cn:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CSCI316-project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24352c6bd30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CSCI316-project\") \\\n",
    ".config(\"spark-master\", \"local\") \\\n",
    ".getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "myManualSchema = StructType([\n",
    "StructField(\"Session_ID\",IntegerType(), True),\n",
    "StructField(\"Timestamp\", TimestampType(), True),\n",
    "StructField(\"Item_ID\", IntegerType(), True),\n",
    "StructField(\"Category\", StringType(), True)])\n",
    "\n",
    "df_fd = spark \\\n",
    ".read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(myManualSchema).load(\"yoochoose-clicks.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_ID: integer (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- Item_ID: integer (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+--------+\n",
      "|Session_ID|           Timestamp|  Item_ID|Category|\n",
      "+----------+--------------------+---------+--------+\n",
      "|         1|2014-04-07 20:51:...|214536502|       0|\n",
      "|         1|2014-04-07 20:54:...|214536500|       0|\n",
      "|         1|2014-04-07 20:54:...|214536506|       0|\n",
      "|         1|2014-04-07 20:57:...|214577561|       0|\n",
      "|         2|2014-04-07 23:56:...|214662742|       0|\n",
      "|         2|2014-04-07 23:57:...|214662742|       0|\n",
      "|         2|2014-04-07 23:58:...|214825110|       0|\n",
      "|         2|2014-04-07 23:59:...|214757390|       0|\n",
      "|         2|2014-04-08 00:00:...|214757407|       0|\n",
      "|         2|2014-04-08 00:02:...|214551617|       0|\n",
      "|         3|2014-04-03 00:17:...|214716935|       0|\n",
      "|         3|2014-04-03 00:26:...|214774687|       0|\n",
      "|         3|2014-04-03 00:30:...|214832672|       0|\n",
      "|         4|2014-04-07 22:09:...|214836765|       0|\n",
      "|         4|2014-04-07 22:26:...|214706482|       0|\n",
      "|         6|2014-04-07 02:58:...|214701242|       0|\n",
      "|         6|2014-04-07 03:02:...|214826623|       0|\n",
      "|         7|2014-04-02 17:38:...|214826835|       0|\n",
      "|         7|2014-04-02 17:39:...|214826715|       0|\n",
      "|         8|2014-04-06 18:49:...|214838855|       0|\n",
      "+----------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transfer the 8-10 digits number to \"B\"\n",
    "from pyspark.sql.functions import regexp_replace,col\n",
    "cateFilter=\"[0-9]{7,10}\"\n",
    "df_FD=df_fd.withColumn(\"Category\",regexp_replace(col(\"Category\"),cateFilter,\"B\"))\n",
    "df_FD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+--------+-----+\n",
      "|Session_ID|           Timestamp|  Item_ID|Category|index|\n",
      "+----------+--------------------+---------+--------+-----+\n",
      "|         1|2014-04-07 20:51:...|214536502|       0|    1|\n",
      "|         1|2014-04-07 20:54:...|214536500|       0|    2|\n",
      "|         1|2014-04-07 20:54:...|214536506|       0|    3|\n",
      "|         1|2014-04-07 20:57:...|214577561|       0|    4|\n",
      "|         2|2014-04-07 23:56:...|214662742|       0|    5|\n",
      "|         2|2014-04-07 23:57:...|214662742|       0|    6|\n",
      "|         2|2014-04-07 23:58:...|214825110|       0|    7|\n",
      "|         2|2014-04-07 23:59:...|214757390|       0|    8|\n",
      "|         2|2014-04-08 00:00:...|214757407|       0|    9|\n",
      "|         2|2014-04-08 00:02:...|214551617|       0|   10|\n",
      "+----------+--------------------+---------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+--------------------+----------+-----------+-----+\n",
      "|newSession_ID|        newTimestamp|newItem_ID|newCategory|index|\n",
      "+-------------+--------------------+----------+-----------+-----+\n",
      "|            1|2014-04-07 20:51:...| 214536502|          0|    0|\n",
      "|            1|2014-04-07 20:54:...| 214536500|          0|    1|\n",
      "|            1|2014-04-07 20:54:...| 214536506|          0|    2|\n",
      "|            1|2014-04-07 20:57:...| 214577561|          0|    3|\n",
      "|            2|2014-04-07 23:56:...| 214662742|          0|    4|\n",
      "|            2|2014-04-07 23:57:...| 214662742|          0|    5|\n",
      "|            2|2014-04-07 23:58:...| 214825110|          0|    6|\n",
      "|            2|2014-04-07 23:59:...| 214757390|          0|    7|\n",
      "|            2|2014-04-08 00:00:...| 214757407|          0|    8|\n",
      "|            2|2014-04-08 00:02:...| 214551617|          0|    9|\n",
      "+-------------+--------------------+----------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create second same dataframe and  change column name in order to join this two tables in the next steps \n",
    "second_df=df_FD.selectExpr(\"Session_ID as newSession_ID\",\"Timestamp as newTimestamp\",\"Item_ID as newItem_ID\",\"Category as newCategory\")\n",
    "\n",
    "#for join,I index two dataframes,and one dataframe index from 0 ,anther one is index from 1,\n",
    "#it can has dislocation by subtracting to calcaulate interval time\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df1 = df_FD.withColumn(\"index\", monotonically_increasing_id()+1)\n",
    "df2 =second_df.withColumn(\"index\", (monotonically_increasing_id()))\n",
    "df1.show(10)\n",
    "df2.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+---------+--------+-------------+--------------------+----------+-----------+\n",
      "|index|Session_ID|           Timestamp|  Item_ID|Category|newSession_ID|        newTimestamp|newItem_ID|newCategory|\n",
      "+-----+----------+--------------------+---------+--------+-------------+--------------------+----------+-----------+\n",
      "|   26|        11|2014-04-03 21:45:...|214821275|       0|           11|2014-04-03 21:45:...| 214821371|          0|\n",
      "|   29|        11|2014-04-03 21:46:...|214821371|       0|           11|2014-04-03 21:53:...| 214717089|          0|\n",
      "|  474|       154|2014-04-03 20:04:...|214560187|       0|          154|2014-04-03 20:05:...| 214716984|          0|\n",
      "|  964|       337|2014-04-04 05:52:...|214820842|       0|          337|2014-04-04 05:56:...| 214826897|          0|\n",
      "| 1677|       564|2014-04-02 21:49:...|214629060|       0|          564|2014-04-02 22:09:...| 214840899|          0|\n",
      "| 1697|       564|2014-04-02 23:26:...|214596647|       0|          564|2014-04-02 23:27:...| 214837558|          0|\n",
      "| 1806|       531|2014-04-02 01:58:...|214748336|       0|          531|2014-04-02 01:59:...| 214717247|          0|\n",
      "| 1950|       638|2014-04-02 23:37:...|214579730|       0|          637|2014-04-02 05:54:...| 214537867|          0|\n",
      "| 2040|       603|2014-04-07 18:20:...|214684513|       0|          602|2014-04-02 23:21:...| 214819562|          0|\n",
      "| 2214|       661|2014-04-02 04:40:...|214832559|       0|          661|2014-04-02 04:41:...| 214819550|          0|\n",
      "+-----+----------+--------------------+---------+--------+-------------+--------------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lefter join two tables,df1 and df2\n",
    "df3 = df1.join(df2, \"index\", \"left_outer\")\n",
    "df3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+---------+--------+-------------+--------------------+----------+-----------+-------------+\n",
      "|index|Session_ID|           Timestamp|  Item_ID|Category|newSession_ID|        newTimestamp|newItem_ID|newCategory|interval_time|\n",
      "+-----+----------+--------------------+---------+--------+-------------+--------------------+----------+-----------+-------------+\n",
      "|   26|        11|2014-04-03 21:45:...|214821275|       0|           11|2014-04-03 21:45:...| 214821371|          0|           28|\n",
      "|   29|        11|2014-04-03 21:46:...|214821371|       0|           11|2014-04-03 21:53:...| 214717089|          0|          385|\n",
      "|  474|       154|2014-04-03 20:04:...|214560187|       0|          154|2014-04-03 20:05:...| 214716984|          0|           42|\n",
      "|  964|       337|2014-04-04 05:52:...|214820842|       0|          337|2014-04-04 05:56:...| 214826897|          0|          222|\n",
      "| 1677|       564|2014-04-02 21:49:...|214629060|       0|          564|2014-04-02 22:09:...| 214840899|          0|         1226|\n",
      "| 1697|       564|2014-04-02 23:26:...|214596647|       0|          564|2014-04-02 23:27:...| 214837558|          0|           49|\n",
      "| 1806|       531|2014-04-02 01:58:...|214748336|       0|          531|2014-04-02 01:59:...| 214717247|          0|           23|\n",
      "| 1950|       638|2014-04-02 23:37:...|214579730|       0|          637|2014-04-02 05:54:...| 214537867|          0|       -63803|\n",
      "| 2040|       603|2014-04-07 18:20:...|214684513|       0|          602|2014-04-02 23:21:...| 214819562|          0|      -417493|\n",
      "| 2214|       661|2014-04-02 04:40:...|214832559|       0|          661|2014-04-02 04:41:...| 214819550|          0|           11|\n",
      "+-----+----------+--------------------+---------+--------+-------------+--------------------+----------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this function is to calculate interval time and put the result into new column\n",
    "import pyspark.sql.functions as F\n",
    "df4=df3.withColumn(\n",
    "    \"interval_time\", \n",
    "    (F.col(\"newTimestamp\").cast(\"long\") - F.col(\"Timestamp\").cast(\"long\")))\n",
    "df4.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+---------+--------+-------------+--------------------+----------+-----------+-------------+\n",
      "|index|Session_ID|           Timestamp|  Item_ID|Category|newSession_ID|        newTimestamp|newItem_ID|newCategory|interval_time|\n",
      "+-----+----------+--------------------+---------+--------+-------------+--------------------+----------+-----------+-------------+\n",
      "|   35|        11|2014-04-03 21:57:...|214826837|       0|           11|2014-04-03 21:57:...| 214819762|          0|           20|\n",
      "|  201|        62|2014-04-07 01:44:...|214826619|       0|           62|2014-04-07 01:45:...| 214746427|          0|           53|\n",
      "|  288|        86|2014-04-02 05:21:...|214648340|       0|           86|2014-04-02 05:21:...| 214648438|          0|           31|\n",
      "|  643|       219|2014-04-01 18:07:...|214725500|       0|          219|2014-04-01 18:10:...| 214839660|          0|          161|\n",
      "| 1219|       389|2014-04-07 03:20:...|214691396|       0|          389|2014-04-07 03:21:...| 214691321|          0|           42|\n",
      "| 1184|       397|2014-04-05 18:19:...|214553540|       0|          397|2014-04-05 18:19:...| 214572538|          0|           26|\n",
      "| 1310|       479|2014-04-03 20:09:...|214820814|       0|          479|2014-04-03 20:11:...| 214746339|          0|          144|\n",
      "| 1521|       496|2014-04-08 03:39:...|214638977|       0|          496|2014-04-08 03:39:...| 214638977|          0|            9|\n",
      "| 1647|       554|2014-04-08 11:36:...|214829312|       0|          554|2014-04-08 11:36:...| 214774685|          0|           33|\n",
      "| 2154|       651|2014-04-02 04:17:...|214718117|       0|          651|2014-04-02 04:17:...| 214690845|          0|           37|\n",
      "+-----+----------+--------------------+---------+--------+-------------+--------------------+----------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#because all seesion_id is continous,so it will come out the condition that last record of one seession_id\n",
    "# subtract the first record of another one session_id,so it is wrong condition,so following step is to filter this\n",
    "\"\"\"\n",
    "example:\n",
    "last column negative number\n",
    "\n",
    "| 1950|       638|2014-04-02 23:37:...|214579730|       0|          637|2014-04-02 05:54:...| 214537867|          0|       -63803|\n",
    "| 2040|       603|2014-04-07 18:20:...|214684513|       0|          602|2014-04-02 23:21:...| 214819562|          0|      -417493|\n",
    "| 2214|       661|2014-04-02 04:40:...|214832559|       0|          661|2014-04-02 04:41:...| 214819550|          0|           11|\n",
    "\n",
    "\"\"\"\n",
    "df5=df4.filter((col(\"Session_ID\") == col(\"newSession_ID\")))\n",
    "df5.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------------+--------+\n",
      "|Session_ID|  Item_ID|complete_interval_time|Category|\n",
      "+----------+---------+----------------------+--------+\n",
      "|        87|214554637|                    13|       0|\n",
      "|       119|214716954|                   794|       0|\n",
      "|       397|214843492|                   127|       0|\n",
      "|       491|214832559|                    42|       0|\n",
      "|       492|214718220|                    93|       0|\n",
      "|       516|214676364|                    59|       0|\n",
      "|       577|214708372|                    48|       0|\n",
      "|       626|214827005|                    54|       0|\n",
      "|       651|214838833|                   252|       0|\n",
      "|       726|214589632|                    23|       0|\n",
      "+----------+---------+----------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.this step is for the condition that in one seesion_id,it has more than or equal two same item_id operations,\n",
    "#because this operations belong to one item,so group it,and sum the interval_time of same item_id records.\n",
    "#2.why I max(\"Category\") here,because I just want to keep Category column after groupby,But I didnt find good\n",
    "#solution to generate it,so I have to do it like this.\n",
    "\"\"\"\n",
    "example:interval time of index 4 and index 5 sum together  \n",
    "                                     item_id            cate  index\n",
    "             2|2014-04-07 23:56:...| 214662742|          0|    4|\n",
    "|            2|2014-04-07 23:57:...| 214662742|          0|    5|\n",
    "|            2|2014-04-07 23:58:...| 214825110|          0|    6|\n",
    "|            2|2014-04-07 23:59:...| 214757390|          0|    7|\n",
    "|            2|2014-04-08 00:00:...| 214757407|          0|    8|\n",
    "|            2|2014-04-08 00:02:...| 214551617|          0|    9|\n",
    "\"\"\"\n",
    "df6=df5.groupBy([col(\"Session_ID\"),col(\"Item_ID\")])\\\n",
    ".agg(F.sum(col(\"interval_time\")).alias(\"complete_interval_time\"),F.max(col(\"Category\")).alias(\"Category\"))\n",
    "df6.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+---------------------------+\n",
      "|Category|count(Item_ID)|sum(complete_interval_time)|\n",
      "+--------+--------------+---------------------------+\n",
      "|       7|        220258|                   47393374|\n",
      "|      11|         44447|                    6591571|\n",
      "|       3|        534948|                   70182484|\n",
      "|       8|         27314|                    5099778|\n",
      "|       0|       9850312|                 1725863516|\n",
      "|       5|        257109|                   62751734|\n",
      "|       B|         46800|                    9880439|\n",
      "|       6|        223263|                   52894611|\n",
      "|       S|       6534387|                 1111613782|\n",
      "|       9|         61836|                   12428756|\n",
      "+--------+--------------+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.so far,we have the complete_interval_time for each user on each same item \n",
    "# 2,group by category,then sum the complete_interval_time,then divide by total number of each user each item \n",
    "df7=df6.groupBy([col(\"Category\")]).agg(F.count(col('Item_ID')),F.sum(col('complete_interval_time')))\n",
    "df7.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+---------------------------+------------------+\n",
      "|Category|count(Item_ID)|sum(complete_interval_time)|      Average_time|\n",
      "+--------+--------------+---------------------------+------------------+\n",
      "|       7|        220258|                   47393374|215.17208909551525|\n",
      "|      11|         44447|                    6591571|148.30182014534165|\n",
      "|       3|        534948|                   70182484| 131.1949647442368|\n",
      "|       8|         27314|                    5099778|186.70930658270484|\n",
      "|       0|       9850312|                 1725863516|175.20902038432894|\n",
      "|       5|        257109|                   62751734| 244.0666565542241|\n",
      "|       B|         46800|                    9880439|211.12049145299144|\n",
      "|       6|        223263|                   52894611|236.91615269883502|\n",
      "|       S|       6534387|                 1111613782|  170.117530841072|\n",
      "|       9|         61836|                   12428756|200.99547189339543|\n",
      "|       1|       1013746|                  201798049| 199.0617462362367|\n",
      "|      10|         41407|                    8073943| 194.9898084864878|\n",
      "|       4|        281139|                   55556414|197.61190727718318|\n",
      "|      12|          9672|                    2787961|288.25072373862696|\n",
      "|       2|        742758|                  157088599|211.49364799840595|\n",
      "+--------+--------------+---------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lastly,calculate the average time\n",
    "final_result=df7.withColumn(\"Average_time\",(col(\"sum(complete_interval_time)\")/col(\"count(Item_ID)\")))\n",
    "final_result.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
